(2a)

f(x) = 1/x
g(x) = 1+x
h(x) = exp(x)
i(x) = -x

σ'(x) = f'(g(h(i(x)))) * 1 * h'(i(x)) * -1
 = -1/(1+exp(-x))^2 * exp(-x) * -1
 = 1/(1+exp(-x)) * [( 1 + exp(-x) - 1 )/(1+exp(-x))]
 = σ(x) * [ 1 - 1/(1+exp(-x)) ]
 = σ(x) * (1 - σ(x))

σ(-x) = 1/(1 + exp(x)) = exp(-x) / [ exp(-x) + exp(-x)*exp(x) ]
 = [ 1 + exp(-x) - 1 ] / (1 + exp(-x))
 = 1 - σ(x)

(2b)

θ is a column vector Cx1
φ = ∑c exp(Θc)
ŷi = exp(θi) / φ
ŷ = softmax(θ) is a column vector Cx1
y is a onehot column vector Cx1 where yk = 1

J(θ) = -y * log(ŷ) = -log(ŷk)

∂J/∂θi = -1/ŷk * [ 0*φ - exp(θk)*exp(θi) ] / φ^2 = ŷi
∂J/∂θk = -1/ŷk * [ exp(θk)*φ - exp(θk)^2 ] / φ^2 = [ exp(θk) - φ ] / φ = ŷk - 1

          ŷ1
          ....
∂J/∂θ = [ ŷk-1 ] = ŷ - y
          ....
          ŷC


(2c)

N is the number of training examples
D is the number of input neurons
H is the number of hidden neurons
C is the number of classes
x is a row vector 1xD
X NxD
W1 (DxH) B1 (H) W2 (HxC) B2 (C)
z1 = X.dot(W1) + B1 # NxD dot DxH = NxH
h = σ(z1) # NxH
z2 = h.dot(W2) + B2 # NxH dot HxC = NxC
ŷ = softmax(z2) # NxC
J(θ) = -∑n (yn * log(ŷn)) / N = -∑n log(ŷn,k) / N

δ3 = ∂J/∂z2 = (ŷ - y) / N # NxC
∂J/∂W2 = ∂z2/∂W2 * δ3 = h.T.dot(δ3) # HxN dot NxC = HxC
∂J/∂B2 = sum(δ3, axis=0) # C
δ2 = ∂J/∂h = δ3 * ∂z2/∂h = δ3.dot(W2.T) # NxC dot CxH = NxH
δ1 = ∂J/∂z1 = δ2 * ∂h/∂z1 = δ2 * σ'(z1) = δ2 * h * (1-h) # NxH
∂J/∂X = δ1 * ∂z1/∂X = δ1.dot(W1.T) # NxH dot HxD = NxD
∂J/∂W1 = ∂z1/∂W1 * δ1 = X.T.dot(δ1) # DxN dot NxH = DxH
∂J/∂B1 = sum(δ1, axis=0) # H


(2d)

DxH + H + HxC + C = (D+1)H + (H+1)C


(3a)

W is the number of words
D is the number of embeddings
ui and vc are column vectors Dx1, output and input respectively
ηi = exp(ui.T.dot(vc))
φ = ∑w ηw
ŷi = ηi / φ
ŷ is a column vector Wx1
y is the onehot expected column vector, yo = 1

      u1.T       u11 u12 ... u1D
U = [ u2.T ] = [ u21 u22 ... u2D ]
      ....       ...............
      uW.T       uW1 uW2 ... uWD

J(o,vc,U) = -y * log(ŷ) = -log(ŷo)

∂J/∂vc = -1/ŷo * [ ηo * uo * φ - ηo * ∑w [ ηw * uw ]  ] / φ^2
 = [ ∑w [ ηw * uw ] - uo * φ  ] / φ = ∑w [ ηw/φ * uw ] - uo
 = ∑w ( ŷw * uw ) - uo
                                     ŷ1
     u11 u21 ... uo1 ... uW1         ....
 = [ u12 u22 ... uo2 ... uW2 ] dot [ ŷo-1 ]
     .......................         ....
     u1D u2D ... uoD ... uWD         ŷW

 = U.T.dot(ŷ - y) # DxW dot Wx1 = Dx1


(3b)

∂J/∂ui = -1/ŷo * [ 0*φ - ηo * ηi * vc ] / φ^2 = ηi * vc / φ = ŷi * vc
∂J/∂uo = -1/ŷo * [ ηo * vc * φ - ηo * ηo * vc ] / φ^2 = vc * [ ηo - φ ] / φ = vc * ( ŷ0 - 1 )

          ∂J/∂u1.T        ŷ1 * vc.T
          ........        ............ 
∂J/∂U = [ ∂J/∂uo.T ] = [ (ŷo-1) * vc.T ]
          ........        ............
          ∂J/∂uW.T        ŷW * vc.T

 = (ŷ - y).dot(vc.T) # Wx1 dot 1xD = WxD

(3c)

      uo1 uo2 uo3 ... uoD       uo.T
ψ = [ u11 u12 u13 ... u1D ] = [ u1.T ]
      ...................       ....
      uK1 uK2 uK3 ... uKD       uK.T

α = [+1] + [-1]*K is a K+1x1 column vector
δ = σ( ψ.dot(vc) * α )
φ = (δ - 1) * α
J(o,vc,U) = -log(σ(uo.dot(vc))) - ∑k [log(σ(-uk.dot(vc)))] = -∑k [log(δk)]
∂J/∂vc = -∑k [ 1/δk * δk * (1 - δk) * αk * uk ] = ∑k [ φk * uk ]

     uo1 u11 u21 ... uK1         φ0
 = [ uo2 u12 u22 ... uK2 ] dot [ φ1 ]
     ...................         ..
     uoD u1D u2D ... uKD         φK

 = ψ.T.dot(φ) # DxK+1 dot K+1x1 = Dx1

For NumPy purposes, it may be desirable for ∂J/∂vc to be flat:

Let ε = ψ.T.dot(φ.reshape(K+1,1)). Then εi = ∑k [ φk * uki ], ε is DxK+1 dot K+1x1 = Dx1, and ε.flatten() = [ ε0 ε1 ... εK ] is flat.
Let β = φ.reshape(1, K+1).dot(ψ) . Then βi = ∑k [ φk * uki ], β is 1xK+1 dot K+1xD = 1xD, and β.flatten() = [ β0 β1 ... βK ] is flat.
And βi = ∑k [ φk * uki ] = εi, β.flatten() = ε.flatten().
From minimal comparison, it seems the calculations for β and ε have similar runtimes. But this is worth further comparison.

∂J/∂ui = -[ 1/δi * δi * (1-δi) * αi * vc ] = φi * vc

          ∂J/∂uo.T       φ0 * vc.T
∂J/∂ψ = [ ∂J/∂u1.T ] = [ φ1 * vc.T ]
          ........       .........
          ∂J/∂uK.T       φK * vc.T

 = φ.dot(vc.T) # K+1x1 dot 1xD = K+1xD

Softmax-CE vs Negative Sampling loss complexity is O(W) / O(K)
