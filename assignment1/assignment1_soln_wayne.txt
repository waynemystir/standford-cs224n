(2a) 

f(x) = 1/x
g(x) = 1+x
h(x) = exp(x)
j(x) = -x

σ(x) = f(g(h(j(x))))

σ'(x) = f'(g(h(j(x)))) * g'(h(j(x))) * h'(j(x)) * j'(x)
 = -1/(1+exp(-x))^2 * 1 * exp(-x) * -1
 = exp(-x) / (1+exp(-x))^2
 = [ exp(-x)/(1+exp(-x)) ] * σ(x)
 = [ (1+exp(-x)-1) / (1+exp(-x)) ] * σ(x)
 = [ 1 - 1/(1+exp(-x)) ] * σ(x)
 = (1 - σ(x)) * σ(x)


(2b)

yi = 1 for the correct label, otherwise 0
φ = Denominator Sum = ∑j exp(θj)
ŷi = softmax(θi) = exp(θi)/φ
J(θ) = -∑j yj*log(ŷj) = -log(ŷk) for some k
Quotient Rule of Derivatives: f(x) = g(x)/h(x)
Then f'(x) = [g'(x)h(x) - h'(x)g(x)] / h(x)^2

Let g(θi) = exp(θk) = c
Let h(θi) = φ
∂J/∂θi = -1/ŷk * [0*φ - exp(θi)*exp(θk)] / φ^2
 = 1/ŷk * exp(θi)/φ * exp(Θk)/φ
 = 1/ŷk * ŷi * ŷk
 = ŷi - 0
 = ŷi - yi for i != k

Let g(θk) = exp(θk)
Let h(θk) = φ
∂J/∂θk = -1/ŷk * [exp(θk)*φ - exp(θk)^2] / φ^2
 = 1/ŷk * { [exp(θk)/φ]^2 - [exp(θk)/φ] }
 = 1/ŷk * ( ŷk^2 - ŷk )
 = ŷk - 1
 = ŷk - yk

So
∂J/∂θ = ŷ - y


(2c)

V is the number of training examples
D is the number of input neurons
H is the number of hidden neurons
C is the number of classes
x (VxD)
W1 (DxH), W2 (HxC), B1 (H), B2 (C) are the weights and biases
z1 = x.dot(W1) + B1 # VxH
h = σ(z1) # VxH
z2 = h.dot(W2) + B2 # VxH dot HxC = VxC
ŷ = softmax(z2)

δ3 = ∂J/∂z2 = ŷ - y (VxC)
δ2 = ∂J/∂h = δ3 * ∂z2/∂h = δ3.dot(W2.T) # VxC dot CxH = VxH
δ1 = ∂J/∂z1 = δ2 * ∂h/∂z1 = δ2 * σ'(z1) = δ2 * sig_grad(h) # VxH
∂J/∂x = δ1 * ∂z1/∂x = δ1.dot(W1.T) # VxH dot HxD = VxD
∂J/∂W1 = δ1 * ∂z1/∂W1 = x.T.dot(δ1) # DxV dot VxH = DxH
∂J/∂B1 = sum(δ1, axis=1, keepdims=False) # H

(2d)

Number of parameters:
W1 DxH
W2 HxC
B1 H
B2 C

DxH + HxC + H + C = H (D+C+1) + C = H(1+D) + C(H+1)


(3a)

W is the number of words
D is the number of embeddings
V = inputVectors (WxD)
U = outputVectors (WxD) = [ u1.T  u2.T  u3.T  ...  uk.T  ...  uW.T ].T
ui (D) column vector
vc (D) column vector
φ = Denominator Sum = ∑w exp(uw.T.dot(vc)) (1)
ŷo = p(o|c) = exp(uo.T.dot(vc)) / φ (1)
ŷ = exp(U.dot(vc) / φ) (W) column vector
y is a onehot vector (W) column vector
J(o,vc,U) = -∑w [yw * log(ŷw)] = -log(ŷo) for some o

∂J/∂vc = -1/ŷo * [ exp(uo.T.dot(vc)) * uo * φ - φ' * exp(uo.T.dot(vc)) ] / φ^2
 = -1/ŷo * exp(uo.T.dot(vc)) / φ * [uo * φ - φ'] / φ
 = -[uo * φ - ∑w [exp(uw.T.dot(vc)) * uw]] / φ
 = ∑w {exp(uw.T.dot(vc)) / φ * uw} - uo
 = ∑w (ŷw * uw) - uo

 = [ ∑w (ŷw*uw1) - uo1   ∑w (ŷw*uw2) - uo2   ∑w (ŷw*uw3) - uo3   ...   ∑w (ŷw*uwD) - uoD ].T

     u11 u21 u31 ... uo1 ... uW1
 = [ u12 u22 u32 ... uo2 ... uW2 ].dot( [ ŷ1  ŷ2  ŷ3  ...  ŷo - 1  ...  ŷW ].T )
     u13 u22 u33 ... uo3 ... uW3
...............................
     u1D u2D u3D ... uoD ... uWD

 = [ u1  u2  u3  ...  uk  ...  uW ].dot( [ ŷ1  ŷ2  ŷ3  ...  ŷo - 1  ...  ŷW ].T )
 = U.T.dot(ŷ - y) # DxW dot Wx1 = Dx1


(3b)

for i != o:  ∂J/∂ui = -1/ŷo * [ 0*φ - ∂φ/∂ui * exp(uo.T.dot(vc)) ] / φ^2
 = 1/ŷo * ∂φ/∂ui / φ * ŷo
 = exp(ui.T.dot(vc)) * vc / φ
 = ŷi * vc

∂J/∂uo = -1/ŷo * [ exp(uo.T.dot(vc)) * vc * φ - exp(uo.T.dot(vc)) * φ' ] / φ^2
 = -1/ŷo * exp(uo.T.dot(vc)) / φ * [vc * φ - φ'] / φ
 = φ' / φ - vc
 = exp(uo.T.dot(vc)) * vc / φ - vc
 = ŷo * vc - vc
 = (ŷo - 1) vc

∂J/∂U = [ ∂J/∂u1.T  ∂J/∂u2.T  ...  ∂J/∂uo.T  ...  ∂J/∂uW.T ].T
 = [ (ŷ1-0)vc.T  (ŷ2-0)vc.T  ...  (ŷo-1)vc.T  ...  (ŷW-0)vc.T ].T

     (ŷ1-0)vc1  (ŷ1-0)vc2  ...  (ŷ1-0)vcD
     (ŷ2-0)vc1  (ŷ2-0)vc2  ...  (ŷ2-0)vcD
.......................................
 = [ (ŷo-1)vc1  (ŷo-1)vc2  ...  (ŷo-1)vcD ]
.......................................
     (ŷW-0)vc1  (ŷW-0)vc2  ...  (ŷW-0)vcD

= (ŷ - y).dot(vc.T) # Wx1 dot 1xD = WxD


(3c)

Negative Sampling Loss = J(o, vc, U) = -log(σ(uo.T.dot(vc))) - ∑k[log(σ(-uk.T.dot(vc)))]
α := [+1] + [-1 for _ in range(K)] is a column vector (K+1x1)
δi := σ(ui.T.dot(vc) * αi) for i=0,1,2,,,K where u0 = uo
∂δi/∂vc = δi * (1-δi) * ui * αi
J = -∑k[log(δk)]
φk := (δk - 1)*αk
Ψ := [ uo.T  u1.T  u2.T  ... uK.T ].T # K+1xD ... the outputWords
∂J/∂vc = -∑k[ 1/δk * δk * (1-δk) * uk * αk ]
 = ∑k [(δk - 1)*αk*uk]
 = ∑k [φk * uk]                                             (1)
 = [ ∑k(φk * uk1)  ∑k(φk * uk2)  ...   ∑k(φk * uk1) ].T

     uo1 u11 u21 ... uK1         φo
     uo2 u12 u22 ... uK2         φ1
 = [ uo3 u13 u23 ... uK3 ] dot [ φ2 ]
     ...................         ..
     uoD u1D u2D ... uKD         φK

 = ψ.T.dot(φ) # DxK+1 dot K+1x1 = Dx1

For our code, it will be more efficient to have ∂J/∂vc flattened to (D,) without having to .T at the end:

 = ∑k [φk * uk.T]   # picking up from (1) above but .T on uk
 = [ ∑k(φk * uk1)  ∑k(φk * uk2)  ...   ∑k(φk * uk1) ]

                          uo1 uo2 uo3 ... uoD
                          u11 u12 u13 ... u1D
 = [φ1 φ2 ... φK+1] dot [ u21 u22 u23 ... u2D ].flatten
                          ...................
                          uK1 uK2 uK3 ... uKD

 = [φ1 φ2 ... φK+1] dot [ uo.T  u1.T  u2.T  ... uK.T ].T.flatten
 = φ.reshape(1,K+1).dot(ψ).flatten # 1xK+1 dot K+1xD = 1xD and then flatten to D

∂J/∂ui = -1/δi * ∂δi/∂ui = -1/δi * δi * (1-δi) * αi * vc = (δi-1)αi * vc = φi*vc

∂J/∂Ψ = [ ∂J/∂uo.T  ∂J/∂u1.T  ∂J/∂u2.T  ...  ∂J/∂uK.T ].T 

     φ0 * vc.T       φ0*vc1  φ0*vc2  φ0*vc3  ...  φ0*vcD
     φ1 * vc.T       φ1*vc1  φ1*vc2  φ1*vc3  ...  φ1*vcD
 = [ φ2 * vc.T ] = [ φ2*vc1  φ2*vc2  φ2*vc3  ...  φ2*vcD ]
     .........       ................................... 
     φK * vc.T       φK*vc1  φK*vc2  φK*vc3  ...  φK*vcD

     φ0
     φ1
 = [ φ2 ] dot [ vc1  vc2  vc3  ...  vcD ]
     ..
     φK

 = φ.dot(vc.T)






