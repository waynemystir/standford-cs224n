(1a)

softmax(x+c)i = exp(xi+c)/∑j[exp(xj+c)] = exp(c)exp(xi)/∑j[exp(c)exp(xj)] = exp(xi)/∑j[exp(xj)] = softmax(x)i

(2a)

σ'(x) = [-1/(1+exp(-x))^2] * exp(-x) * -1 = exp(-x) / (1+exp(-x))^2
 = [1+exp(-x)-1] / (1+exp(-x))^2 = (1+exp(-x)) / (1+exp(-x))^2 - 1/(1+exp(-x))^2
 = 1/(1+exp(-x)) - σ(x)^2 = σ(x) - σ(x)^2
 = σ(x)(1 - σ(x))

EXTRA: Let's show that σ(-x) = 1 - σ(x)
1-σ(x) = [1+exp(-x) - 1]/(1+exp(-x)) = [exp(-x) / (1+exp(-x))] * [exp(x)/exp(x)]
 = [exp(-x)exp(x)] / [(1+exp(-x))exp(x)] = exp(x-x)/[exp(x) + exp(-x)exp(x)]
 = 1/(exp(x) + 1) = σ(-x)


(2b)

ŷ = softmax(θ), where θ is a vector
∑ = ∑j[exp(θj)]
ŷi = exp(θi)/∑
J = CE(y,ŷ) = -∑i[yilog(ŷi)] = -log(ŷk), where k is the desired label

∂J/∂θi = -1/ŷk * -exp(θk)/∑^2 * exp(θi)
 = ŷk/ŷk * ŷi = ŷi for i != k

∂J/∂θk = -1/ŷk * [∑ - exp(θk)] / ∑^2 * exp(θk)   ** Note below
 = -∑/exp(θk) * (1 - ŷk) * 1/∑ * exp(θk)
 = ŷk - 1

** This follows from: if f(x) = x/(x+c), then f'(x) = c/(x+c)^2:
Let g(x) = x and h(x) = x+c. Then the quotient rule of derivatives tells us
f'(x) = [g'(x)h(x) - h'(x)g(x)]/[h(x)]^2
 = (x+c - x)/(x+c)^2
 = c/(x+c)^2

Then set c = ∑,j!=k = ∑ - exp(θk) and x = exp(θk) to get
∂ŷk/∂exp(θk) = [∑ - exp(θk)] / ∑^2
Hence ∂ŷk/∂θk = ∂ŷk/∂exp(θk) * ∂exp(θk)/∂θk = [∑ - exp(θk)] / ∑^2 * exp(θk)

And finally ∂J/∂θ = [ ŷ1-0  ŷ2-0  ...  ŷk-1  ... ŷw-0 ].T = ŷ - y


Or this might be cleaner:

Let's first get the Jacobian of the softmax.
Let g(θi) = exp(θi) and ∑ = h(θi) = ∑j[exp(θj)] so that ŷi = g(θi)/h(θi)
and the quotient rule of derivatives gives us

∂ŷi/∂θi = [exp(θi)*∑ - exp(θi)^2] / ∑^2 = ŷi - ŷi^2 = ŷi(1 - ŷi)
∂ŷj/∂θi = [0*∑ - exp(θj)^2] / ∑^2 = -ŷj^2

And

∂J/∂θk = ∂J/∂ŷk * ∂ŷk/∂θk = -1/ŷk * ŷ(1 - ŷk) = ŷk - 1
∂J/∂θi = ∂J/∂ŷk * ∂ŷk/∂θi = -1/ŷk * -ŷk^2 = ŷk for i != k

And finally ∂J/∂θ = [ ŷ1-0  ŷ2-0  ...  ŷk-1  ... ŷw-0 ] = ŷ - y

And incidentally the Jacobian of the softmax is 

  ŷ1(1-ŷ1)    -ŷ1^2      -ŷ1^2      -ŷ1^2      -ŷ1^2
  -ŷ2^2       ŷ2(1-ŷ2)   -ŷ2^2      -ŷ2^2      -ŷ2^2
[ -ŷ3^2       -ŷ3^2      ŷ3(1-ŷ3)   -ŷ3^2      -ŷ3^2   ]
  -ŷ4^2       -ŷ4^2      -ŷ4^2      ŷ4(1-ŷ4)   -ŷ4^2
  -ŷ5^2       -ŷ5^2      -ŷ5^2      -ŷ4^2      ŷ5(1-ŷ5)



(2c)

δ3 = ∂J/∂θ = ŷ - y # 1x5
δ2 = ∂J/∂h = ∂J/∂θ * ∂θ/∂h = (ŷ - y).dot(W2.T) # 1x5 dot 5x3 = 1x3
δ1 = ∂J/∂z1 = δ2 * ∂h/∂z1 = δ2 * σ'(z1)
∂J/∂x = δ1 * ∂z1/∂x = δ1.dot(W1.T) # 1x3 dot 3x5 = 1x5
∂J/∂W1 = ∂z1/∂W1 * δ1 = x.T.dot(δ1) # 5x1 dot 1x3 = 5x3
∂J/∂B1 = δ1 # 1x3


(2d) Neural Networks - number of parameters in a 2 layer network

Concrete example:

L1, X has D=5 dims, Hidden has H=3 neurons
*******************************************************
        1x5        *       5x3       +     1x3  

                       W11 W12 W13
                       W21 W22 W23
  x1 x2 x3 x4 x5   *   W31 W32 W33   +   b1 b2 b3
                       W41 W42 W43
                       W51 W52 W53

So DH + H = (D+1)H parameters

L2, H=3 neurons, Output has Y=5 dims
*******************************************************
     1x3       *          3x5            +     1x5

                   W11 W12 W13 W14 W15
                   W21 W22 W23 W24 W25
   h1 h2 h3    *   W31 W32 W33 W34 W35   +   b1 b2 b3 b4 b5
                   W41 W42 W43 W44 W45
                   W51 W52 W53 W54 W55

So HY + Y = (H+1)Y parameters.

Or we can flip the multiplication:

L1, X has D=5 dims, Hidden has H=3 neurons
*******************************************************
   3x5                *      5x1           +   3x1  

W11 W12 W13 W14 W15
W21 W22 W23 W24 W25   *   x1 x2 x3 x4 x5   +   b1 b2 b3
W31 W32 W33 W34 W35

So DH + H = (D+1)H parameters

L2, H=3 neurons, Output has Y=5 dims
*******************************************************
   5x3                *     3x1      +   5x1

W11 W12 W13
W21 W22 W23
W31 W32 W33           *   h1 h2 h3   +   b1 b2 b3 b4 b5
W41 W42 W43
W51 W52 W53

So HY + Y = (H+1)Y parameters.

Generally: Suppose X has D dims, Hidden has H neurons, and Output has Y dims.

Then W1 has DxH parameters and B1 has H parameters. So that's D*H + H = (D+1)*H parameters.

Similarly, W2 has HxY parameters and B2 has Y parameters. So that's (H+1)*Y parameters.

Hence, (D+1)*H + (H+1)*Y parameters.


(3a)

https://stats.stackexchange.com/a/277590

|W| is the number of words in the vocabulary
ui and vj are column vectors of shape Dx1 (D=dimensions of embeddings)
y is the one-hot column vector of shape |W|x1
ŷ is the softmax prediction column vector of shape |W|x1
U = [u1,u2,...,uk,...uW] is the matrix composed of uk column vectors
ŷi = exp(ui.T.dot(vc)) / Φ
Φ = Denominator Sum = ∑w [exp(uw.T.dot(vc))]
Cross entropy loss: J = −∑w[ywlog(ŷw)]
o is the index of the labeled word, so that yo = 1 and yw = 0 for w!=o


J = -∑w [yw * log(ŷw)] # definition
 = -log(ŷo) # yw = 1 for w = o, otherwise yw = 0
 = -[uo.T.dot(vc) - log(Φ)]
 = log(Φ) - uo.T.dot(vc)

∂J/∂vc = 1/Φ * ∑w [exp(uw.T.dot(vc)) * uw] - uo
 = ∑w [exp(uw.T.dot(vc))/Φ * uw] - uo
 = ∑w [ŷw*uw] - uo

Or

∂J/∂vc = ∑w [∂J/∂xw * ∂xw/∂vc]
where xw = uw.T.dot(vc)
The sum is needed because all xi's depend on vc and the ŷi's (which depend on the xi's) are summed together in J.

We know that ∂J/∂xw = ŷw - yw (from problem 2b above)
And that ∂xw/∂vc = uw
So that ∂J/∂vc = ∑w[(ŷw - yw)uw] = ∑w [ŷw*uw] - uo


Now we show that uo = U.dot(y) # Dx1

Suppose |W|=4, D=3, and o=3.

      u11 u12 u13 u14
U = [ u21 u22 u23 u24 ]
      u31 u32 u33 u34

      y1
      y2
y = [ y3 ], y3 = 1, y1,2,4 = 0
      y4


U.dot(y) = [u13 u23 u33].T

      u13
  = [ u23 ]
      u33

  = u3


And lastly we show that U.dot(ŷ) = ∑ (ŷw uw)

      ŷ1
      ŷ2
ŷ = [ ŷ3 ]
      ŷ4 



             u11*ŷ1 + u12*ŷ2 + u13*ŷ3 + u14*ŷ4
U.dot(ŷ) = [ u21*ŷ1 + u22*ŷ2 + u23*ŷ3 + u24*ŷ4 ]
             u31*ŷ1 + u32*ŷ2 + u33*ŷ3 + u34*ŷ4

     ŷ1*u11       ŷ2*u12       ŷ3*u13       ŷ4*u14
 = [ ŷ1*u21 ] + [ ŷ2*u22 ] + [ ŷ3*u23 ] + [ ŷ4*u24 ]
     ŷ1*u31       ŷ2*u32       ŷ3*u33       ŷ4*u34


 = ∑w (ŷw uw)

So U.dot(ŷ-y) = U.dot(ŷ) - U.dot(y) = ∑ (ŷw uw) - uo


(3b) 

J = -∑w [yw * log(ŷw)] # definition
 = -log(ŷo) # yw = 1 for w = o, otherwise yw = 0
 = -[uo.T.dot(vc) - log(Φ)]
 = log(Φ) - uo.T.dot(vc)

∂J/∂uo = 1/Φ * exp(uo.T.dot(vc)) * vc - vc
 = ŷo * vc - vc
 = (ŷo - 1) vc

∂J/∂ui = 1/Φ * exp(ui.T.dot(vc)) * vc
 = ŷi * vc

∂J/∂U = [ ∂J/∂u1  ∂J/∂u2  ∂J/∂u3  ∂J/∂u4 ]
 = [ ŷ1*vc  ŷ2*vc  (ŷ3 - 1)*vc  ŷ4*vc]

     ŷ1*vc1  ŷ2*vc1  (ŷ3 - 1)*vc1  ŷ4*vc1
 = [ ŷ1*vc2  ŷ2*vc2  (ŷ3 - 1)*vc2  ŷ4*vc2 ]
     ŷ1*vc3  ŷ2*vc3  (ŷ3 - 1)*vc3  ŷ4*vc3

 = vc.dot((ŷ - y).T)
 = vc ⊗ (ŷ - y)


(3c)

Negative Sampling Loss = J(o, vc, U) = -log(σ(uo.T.dot(vc))) - ∑k[log(σ(-uk.T.dot(vc)))]

∂(Left Side)/∂vc = -1/(σ(uo.T.dot(vc))) * σ'(uo.T.dot(vc)) * uo
 = -[σ(1-σ)/σ] * uo = (σ(uo.T.dot(vc)) - 1)uo
∂(Right Size)/∂vc = -∑k[(σ(-uk.T.dot(vc)) - 1)uk]
∂J/∂vc = (σ(uo.T.dot(vc)) - 1)uo - ∑k[(σ(-uk.T.dot(vc)) - 1)uk]

∂J/∂uo = -1/σ * σ' * vc = (σ(uo.T.dot(vc))-1)vc
∂J/∂uk = -(1/σ * σ' * -vc) = -(σ(-uk.T.dot(vc)) - 1)vc for all k=1,2,...,K

Why is Negative Sampling so much more efficient?
Softmax-CE Loss vs Negative Sampling Loss runtime is O(|V|) vs O(K).




